name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  python-tests:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run Python tests with coverage
      run: |
        python -m pytest tests/ -v \
          --cov=app \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junit-xml=test-results.xml
        
    - name: Generate coverage summary
      run: |
        coverage report --show-missing > coverage-summary.txt
        echo "=== COVERAGE SUMMARY ===" >> coverage-summary.txt
        echo "Generated at: $(date)" >> coverage-summary.txt
        echo "Commit: ${{ github.sha }}" >> coverage-summary.txt
        echo "Branch: ${{ github.ref_name }}" >> coverage-summary.txt
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
        
    - name: Upload test results as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ github.run_number }}
        path: |
          test-results.xml
          coverage-summary.txt
          htmlcov/
          coverage.xml
        retention-days: 90
        
    - name: Run linting
      run: |
        pip install flake8 black
        flake8 app/ --max-line-length=88 --extend-ignore=E203,W503,E501,E302,E303,E304,E305,E114,E116,E117,E261,E262,E266,F401,F541,E301,E127,W291 || echo "Linting issues found but continuing..."
        black --check app/ || echo "Black formatting issues found but continuing..."
        
  test-generation:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Test AI provider detection
      run: |
        python -m pytest tests/test_ai_providers.py -v
        
    - name: Test enhanced generation
      run: |
        python -m pytest tests/test_enhanced_ai.py -v
        
    - name: Test domain data generation
      run: |
        python -m pytest tests/test_domain_data.py -v
        
  integration-tests:
    runs-on: ubuntu-latest
    needs: [python-tests, test-generation]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Start test server
      run: |
        # Start the server in the background with proper logging
        python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --log-level info > server.log 2>&1 &
        SERVER_PID=$!
        
        # Wait for server to start and be ready
        echo "Waiting for server to start..."
        for i in {1..30}; do
          if curl -f http://localhost:8000/health > /dev/null 2>&1; then
            echo "Server is ready after $i seconds"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "Server failed to start after 30 seconds"
            echo "Server log:"
            cat server.log
            exit 1
          fi
          sleep 1
        done
        
        # Verify server is running
        if ! ps -p $SERVER_PID > /dev/null; then
          echo "Server process is not running"
          echo "Server log:"
          cat server.log
          exit 1
        fi
        
        echo "Server started successfully with PID $SERVER_PID"
        
    - name: Run integration tests
      run: |
        # Test health endpoint
        echo "Testing health endpoint..."
        curl -f http://localhost:8000/health || exit 1
        
        # Test API endpoint with base64 encoded spec
        echo "Testing API generation endpoint..."
        echo '{"openapi":"3.0.0","info":{"title":"Test API","version":"1.0.0"},"paths":{"\/test":{"get":{"responses":{"200":{"description":"OK"}}}}}}' | base64 -w 0 > /tmp/spec.b64
        SPEC_B64=$(cat /tmp/spec.b64)
        
        # Test the generate endpoint
        response=$(curl -s -w "%{http_code}" -X POST http://localhost:8000/api/generate \
          -H "Content-Type: application/json" \
          -d "{\"openapi\": \"$SPEC_B64\", \"outputs\": [\"json\"], \"casesPerEndpoint\": 2}")
        
        http_code="${response: -3}"
        body="${response%???}"
        
        echo "Response HTTP code: $http_code"
        echo "Response body: $body"
        
        if [ "$http_code" != "200" ]; then
          echo "API test failed with HTTP code $http_code"
          exit 1
        fi
        
        echo "Integration tests completed successfully"
        
    - name: Cleanup test server
      if: always()
      run: |
        # Kill the test server
        if [ ! -z "$SERVER_PID" ]; then
          kill $SERVER_PID 2>/dev/null || true
        fi
        # Also kill any uvicorn processes
        pkill -f "uvicorn app.main:app" || true

  e2e-functional-test:
    runs-on: ubuntu-latest
    needs: [python-tests, test-generation]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Set up Java
      uses: actions/setup-java@v4
      with:
        distribution: 'temurin'
        java-version: '11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run end-to-end functional test
      run: |
        python -m pytest tests/test_e2e_functional.py::test_complete_user_experience \
          -v -s --log-cli-level=INFO \
          --junit-xml=e2e-test-results.xml
        
    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      with:
        name: e2e-test-results-${{ github.run_number }}
        path: e2e-test-results.xml
        retention-days: 90
