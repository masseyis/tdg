#!/usr/bin/env python3
"""
CRITICAL: Post-deployment end-to-end test for the test generation service

‚ö†Ô∏è  IMPORTANT NOTES TO SELF:
1. ALL TESTS MUST RUN WITH COVERAGE AND PASS BEFORE DEPLOYMENT
2. The e2e test is the most important - it validates the complete user journey
3. This post-deploy test must fully mimic user behavior, just like the e2e test does
4. This test must ALWAYS run against the deployed site, never locally
5. This test validates the deployed service works exactly like local development

This test validates the complete user experience against the live deployment:
1. Connects to the deployed Fly.io service (https://tdg-mvp.fly.dev)
2. Uploads an OpenAPI spec via the web UI
3. Generates test files for Java, Python, and Node.js
4. Compiles and runs the generated tests against a mock service
5. Validates that all frameworks work correctly

This ensures the deployed version is working correctly and provides
the same user experience as the local development environment.

IMPORTANT: This test reuses the same WebUIDriver and test logic as the e2e test
to ensure we don't have duplicate code to maintain. The only difference is the URL.
"""

import asyncio
import io
import json
import logging
import os
import subprocess
import tempfile
import time
import zipfile
from pathlib import Path
from typing import Dict, Any, Optional

import httpx
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

from tests.mock_service import MockService

# Import the same classes from e2e test to reuse code
# ‚ö†Ô∏è  CRITICAL: This test MUST use the same test runner classes as the e2e test!
# ‚ö†Ô∏è  Any changes to WebUIDriver, JavaTestRunner, PythonTestRunner, or NodeTestRunner
# ‚ö†Ô∏è  in test_e2e_functional.py will automatically apply to this test.
# ‚ö†Ô∏è  This ensures both tests stay in sync and validate the same behavior.
from tests.test_e2e_functional import WebUIDriver, JavaTestRunner, PythonTestRunner, NodeTestRunner

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
DEPLOYED_URL = "https://tdg-mvp.fly.dev"
MOCK_SERVICE_PORT = 8082


@pytest.mark.asyncio
@pytest.mark.post_deploy
@pytest.mark.skipif(
    not (os.getenv('CI') == 'true' or os.getenv('GITHUB_ACTIONS') == 'true'),
    reason="This test only runs in CI/CD against the deployed site"
)
async def test_deployed_service_complete_user_experience():
    """
    CRITICAL: Post-deployment test against live site
    
    This test reuses the same code as the e2e test but runs against the deployed service:
    1. Connects to https://tdg-mvp.fly.dev
    2. Tests complete user journey: File upload ‚Üí Generation ‚Üí Download ‚Üí Test execution
    3. Validates deployed service works exactly like local e2e test
    4. Must always run against the deployed site, never locally
    
    IMPORTANT: This test validates the deployed service is working correctly
    and provides the same user experience as the local development environment.
    
    NOTE: This test is automatically skipped during local development and only
    runs in CI/CD environments to validate the deployed service.
    """
    
    # Step 1: Start the mock API service for testing generated code
    logger.info("üåê Starting mock API service...")
    mock_service = MockService(Path("tests/samples/petstore-minimal.yaml"), port=MOCK_SERVICE_PORT)
    mock_service.start()
    
    # Step 2: Start the web UI driver pointing to deployed service
    logger.info("üåê Starting web UI driver for deployed service...")
    ui_driver = WebUIDriver(DEPLOYED_URL)
    
    try:
        # Wait for services to be ready
        logger.info("‚è≥ Waiting for services to be ready...")
        time.sleep(5)
        
        # Verify deployed service is responding
        logger.info("üîç Verifying deployed service health...")
        with httpx.Client() as http_client:
            response = http_client.get(f"{DEPLOYED_URL}/health")
            assert response.status_code == 200, f"Deployed service should be healthy, got {response.status_code}"
        logger.info("‚úÖ Deployed service is responding")
        
        # Verify mock service is responding
        logger.info("üîç Verifying mock service health...")
        with httpx.Client() as http_client:
            response = http_client.get(f"http://localhost:{MOCK_SERVICE_PORT}/openapi.json")
            assert response.status_code == 200, f"Mock service should serve OpenAPI spec, got {response.status_code}"
        logger.info("‚úÖ Mock service is responding")
        
        # Step 3: Start browser and navigate to deployed app
        logger.info("üåê Starting browser...")
        if not ui_driver.start_browser():
            raise AssertionError("Browser failed to start")
        logger.info("‚úÖ Browser started successfully")
        
        logger.info("üß≠ Navigating to deployed app page...")
        if not ui_driver.navigate_to_app():
            raise AssertionError("Failed to navigate to deployed app page")
        logger.info("‚úÖ Successfully navigated to deployed app page")
        
        # Step 4: Upload OpenAPI spec and generate tests
        logger.info("üìù Generating tests via deployed web UI...")
        spec_file = Path("tests/samples/petstore-minimal.yaml")
        
        if not ui_driver.upload_spec_file(spec_file):
            raise AssertionError("Failed to upload spec file to deployed service")
        logger.info("‚úÖ Spec file uploaded successfully to deployed service")
        
        if not ui_driver.set_test_parameters(cases_per_endpoint=5, domain_hint="petstore"):
            raise AssertionError("Failed to set test parameters on deployed service")
        logger.info("‚úÖ Test parameters set successfully on deployed service")
        
        if not ui_driver.submit_form():
            raise AssertionError("Failed to submit form to deployed service")
        logger.info("‚úÖ Form submitted successfully to deployed service")
        
        # Step 5: Wait for generation to complete via the UI (proper e2e testing)
        logger.info("‚è≥ Waiting for test generation to complete via deployed UI...")
        
        # The UI should handle the form submission and show progress/completion
        # This is the proper e2e test - we're testing the complete user journey
        if not ui_driver.wait_for_generation_complete():
            raise AssertionError("Test generation did not complete via deployed UI")
        logger.info("‚úÖ Test generation completed successfully via deployed UI")
        
        # Step 6: Get the downloaded ZIP file
        logger.info("üì¶ Getting downloaded ZIP file from deployed service...")
        zip_file_path = ui_driver.get_downloaded_file_path()
        
        # If browser download failed, use synchronous endpoint as fallback
        # ‚ö†Ô∏è  CRITICAL: This uses the same fallback logic as the e2e test!
        # ‚ö†Ô∏è  Any changes to the fallback logic in e2e test should be applied here too.
        if not zip_file_path:
            logger.warning("‚ö†Ô∏è  Browser download failed, using synchronous endpoint as fallback...")
            
            # Since the UI generation completed successfully, we can use the synchronous endpoint
            # to generate the same test cases and get the ZIP file
            zip_file_path = ui_driver.generate_via_sync_endpoint(spec_file)
            
            if not zip_file_path:
                raise AssertionError("Failed to get downloaded ZIP file from deployed service via both browser and synchronous endpoint")
        
        logger.info(f"‚úÖ ZIP file downloaded from deployed service: {zip_file_path}")
        
        # Step 7: Extract and run the generated tests
        logger.info("üîç Extracting and running generated tests from deployed service...")
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Extract ZIP file
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                zip_ref.extractall(temp_path)
            
            logger.info(f"üì¶ Extracted test files to: {temp_path}")
            
            # Test Java framework
            logger.info("‚òï Testing Java framework from deployed service...")
            java_dir = temp_path / "artifacts" / "junit"
            if java_dir.exists():
                java_runner = JavaTestRunner()
                java_success = java_runner.run_tests(java_dir, f"http://localhost:{MOCK_SERVICE_PORT}")
                assert java_success, "Java tests should compile and run against the mock service"
                logger.info("‚úÖ Java framework test completed")
            else:
                logger.warning("‚ö†Ô∏è  Java artifacts not found in generated ZIP from deployed service")
            
            # Test Python framework
            logger.info("üêç Testing Python framework from deployed service...")
            python_dir = temp_path / "artifacts" / "python"
            if python_dir.exists():
                python_runner = PythonTestRunner()
                python_success = python_runner.run_tests(python_dir, f"http://localhost:{MOCK_SERVICE_PORT}")
                assert python_success, "Python tests should run against the mock service"
                logger.info("‚úÖ Python framework test completed")
            else:
                logger.warning("‚ö†Ô∏è  Python artifacts not found in generated ZIP from deployed service")
            
            # Test Node.js framework
            logger.info("üü¢ Testing Node.js framework from deployed service...")
            node_dir = temp_path / "artifacts" / "nodejs"
            if node_dir.exists():
                node_runner = NodeTestRunner()
                node_success = node_runner.run_tests(node_dir, f"http://localhost:{MOCK_SERVICE_PORT}")
                assert node_success, "Node.js tests should run against the mock service"
                logger.info("‚úÖ Node.js framework test completed")
            else:
                logger.warning("‚ö†Ô∏è  Node.js artifacts not found in generated ZIP from deployed service")
        
        # Step 8: Verify results
        logger.info("‚úÖ All tests passed! Post-deployment test successful.")
        logger.info("üéâ Deployed service works exactly like local development!")
        
    except Exception as e:
        logger.error(f"‚ùå Post-deployment test failed: {e}")
        raise
    finally:
        # Clean up
        try:
            if 'ui_driver' in locals():
                ui_driver.stop_browser()
            if 'mock_service' in locals():
                mock_service.stop()
            logger.info("üßπ Cleanup completed")
        except Exception as cleanup_error:
            logger.warning(f"‚ö†Ô∏è  Cleanup error: {cleanup_error}")
